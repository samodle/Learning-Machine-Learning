{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38d7dd7c",
   "metadata": {},
   "source": [
    "Homework 6 <br>\n",
    "Sam Odle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea4a7bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8ccc71",
   "metadata": {},
   "source": [
    "### Download the Fashion MNIST dataset\n",
    "\n",
    "And convert it into a format (a tensor) that pytorch understands. A tensor can be thought of as matrix (you get to define the dimensions).\n",
    "\n",
    "Feed the raw images and their labels into a DataLoader that will serve them up in batches to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eab35df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to /Users/samodle/.pytorch/F_MNIST_data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11.6%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "31.8%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "51.3%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "71.2%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "91.5%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/samodle/.pytorch/F_MNIST_data/FashionMNIST/raw/train-images-idx3-ubyte.gz to /Users/samodle/.pytorch/F_MNIST_data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to /Users/samodle/.pytorch/F_MNIST_data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/samodle/.pytorch/F_MNIST_data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to /Users/samodle/.pytorch/F_MNIST_data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to /Users/samodle/.pytorch/F_MNIST_data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "68.7%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a transform for the data\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539ed5cc",
   "metadata": {},
   "source": [
    "### View a sample image and label\n",
    "We need to reshape the tensor and feed those pixels into a display library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99a03498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label of image tensor(6)\n",
      "(1, 28, 28)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tensor_image, label = next(iter(trainloader)) # returns a batch of images\n",
    "print(\"Label of image\", label[0])\n",
    "first_image = numpy.array(tensor_image, dtype='float')[0] # get the first image in the batch\n",
    "print(first_image.shape)\n",
    "pixels = first_image.reshape((28, 28))\n",
    "plt.imshow(pixels, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fb212b",
   "metadata": {},
   "source": [
    "### Define your model\n",
    "We will have three hidden layers, an input later, and then an output layer. All nodes are Linear nodes (using linear activation).\n",
    "\n",
    "The input layer takes 784 == 28x28 features, corresponding to our greyscale input images.\n",
    "The three hidden layers have 256, 128, and 64 neurons respectively; each previous layer's neurons all feed into each neuron in the next layer (\"fully connected\").\n",
    "The final layer has ten neurons, one representing each of the ten possible classes for this model.\n",
    "\n",
    "Under the forward method, we define the activation function for each later: we're using ReLu for all the hidden layers. In the last layer, we're switching to softmax so we can get the probabilities for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6335f4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your network architecture here\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # make sure input tensor is flattened\n",
    "        x = x.view(x.shape[0], -1) # -1 means we'll let the computer figure out how many rows to include\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.softmax(self.fc4(x), dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644bef2c",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "We choose a learning rate (lr) and some number of epochs, with the goal that the loss is going down each epoch until model convergence on the minimum loss (estimated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7651a943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0\n",
      "\tTraining loss: 1.8687759175229428\n",
      "In epoch 1\n",
      "\tTraining loss: 1.7347458057057883\n",
      "In epoch 2\n",
      "\tTraining loss: 1.7260644289730454\n",
      "In epoch 3\n",
      "\tTraining loss: 1.722333353465554\n",
      "In epoch 4\n",
      "\tTraining loss: 1.6928342457519157\n",
      "In epoch 5\n",
      "\tTraining loss: 1.6837520307061007\n",
      "In epoch 6\n",
      "\tTraining loss: 1.6762444464636763\n",
      "In epoch 7\n",
      "\tTraining loss: 1.6737940480459983\n",
      "In epoch 8\n",
      "\tTraining loss: 1.670207276654396\n",
      "In epoch 9\n",
      "\tTraining loss: 1.6684166363307409\n",
      "In epoch 10\n",
      "\tTraining loss: 1.6669433067348212\n",
      "In epoch 11\n",
      "\tTraining loss: 1.6676799194899194\n",
      "In epoch 12\n",
      "\tTraining loss: 1.664239117458685\n",
      "In epoch 13\n",
      "\tTraining loss: 1.6618965090210758\n",
      "In epoch 14\n",
      "\tTraining loss: 1.660264431413557\n"
     ]
    }
   ],
   "source": [
    "# store train and validation loss in separate arrays, one for each epoch, and graph this!! be careful about eval vs train mode\n",
    "\n",
    "# Create the custom network above, define the loss criterion, and choose an optimizer to learn weights\n",
    "model = Classifier()\n",
    "loss_criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.4)\n",
    "\n",
    "# Train the network here\n",
    "epochs = 15\n",
    "\n",
    "for e in range(epochs):\n",
    "    print(\"In epoch\", e)\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader: # processing one batch at a time\n",
    "        predictions = model(images) # predict labels\n",
    "        loss = loss_criterion(predictions, labels) # calculate the loss\n",
    "        \n",
    "        # BACK PROPAGATION OF LOSS to generate updated weights\n",
    "        optimizer.zero_grad() # pytorch accumulates gradients from previous backwards\n",
    "                              # passes by default -- we want to zero them out;\n",
    "                              # you can read online why they have this implementation choice\n",
    "        loss.backward() # compute gradients by using the predictions' grad_fn\n",
    "                        # that was passed to loss_criterion() above -- this is how it\n",
    "                        # knows what model parameters need updating eventually\n",
    "                        # (this is confusing IMO and not obvious to those used to OOP)\n",
    "        optimizer.step() # using gradients just calculated for model parameters, \n",
    "                         # update the weights via the optimizer (which was init with those \n",
    "                         # model parameters)\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    print(f\"\\tTraining loss: {running_loss/len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381fef4d",
   "metadata": {},
   "source": [
    "### Making predictions\n",
    "Once we've trained the model, we can grab an image and pull out information about its predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "63311c66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target label of image:  tensor(8)\n",
      "(1, 28, 28)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.8244e-10, 8.7632e-29, 1.4679e-12, 6.6328e-18, 1.2958e-15, 4.4984e-11,\n",
      "         8.2767e-18, 4.9117e-30, 1.0000e+00, 7.5048e-30]],\n",
      "       grad_fn=<SoftmaxBackward>)\n",
      "max_prob 1.0\n",
      "Predicted label:  8\n",
      "Target label:  8\n",
      "Successful prediction!\n"
     ]
    }
   ],
   "source": [
    "# Test the trained model\n",
    "tensor_image, label = next(iter(testloader)) # returns a batch of images\n",
    "print(\"Target label of image: \", label[0])\n",
    "first_image = numpy.array(tensor_image, dtype='float')[0] # get the first image in the batch\n",
    "print(first_image.shape)\n",
    "pixels = first_image.reshape((28, 28))\n",
    "plt.imshow(pixels, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "prediction = model(tensor_image[0])\n",
    "print(prediction)\n",
    "prediction = prediction.detach().numpy()\n",
    "max_prob = max(list(numpy.array(prediction)[0]))\n",
    "print('max_prob', max_prob)\n",
    "predicted_label = list(prediction[0]).index(max_prob)\n",
    "print(\"Predicted label: \", predicted_label)\n",
    "\n",
    "tgt_lbl = label[0].item()\n",
    "print(\"Target label: \", tgt_lbl)\n",
    "\n",
    "if tgt_lbl == predicted_label:\n",
    "    print(\"Successful prediction!\")\n",
    "else:\n",
    "    print(\"Not quite...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Accuracy for each batch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++326+++6+2+++++++6+++++0++++6+++++++++++2++++9+0++6+++++++++6++\n",
      "Accuracy: 79.69% for 64 images.\n"
     ]
    }
   ],
   "source": [
    "# Test the trained model\n",
    "tensor_image, label = next(iter(testloader)) # returns a batch of images\n",
    "\n",
    "correct = 0\n",
    "incorrect = 0\n",
    "\n",
    "for i in range(len(label.tolist())):\n",
    "    first_image = numpy.array(tensor_image, dtype='float')[i] # get the first image in the batch\n",
    "\n",
    "    prediction = model(tensor_image[i])\n",
    "    prediction = prediction.detach().numpy()\n",
    "    max_prob = max(list(numpy.array(prediction)[0]))\n",
    "    predicted_label = list(prediction[0]).index(max_prob)\n",
    "\n",
    "    if label[i].item() == predicted_label:\n",
    "        correct += 1\n",
    "        print('+', end = '')\n",
    "    else:\n",
    "        incorrect += 1\n",
    "        print(predicted_label, end = '') #let's take a look at what we're missing\n",
    "\n",
    "accuracy = round(correct * 100 / (correct + incorrect), 2)\n",
    "print('')\n",
    "print(f'Accuracy: {accuracy}% for {correct + incorrect} images.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "f4c63d60",
   "metadata": {},
   "source": [
    "### Playground\n",
    "\n",
    "What happens when you: <br>\n",
    " <br>**1. Use a much smaller learning rate for the model?** <br>\n",
    "    Using the initial learning rate of 0.3, the model settles into a loss of ~1.7 after 7/8 epochs. <br>\n",
    "\n",
    "\n",
    "<br> <br>**2. Feed in the entire test dataset to the model, to make predictions? Could you write code to do this, and measure your model performance?** <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 6/3"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/1x/4x80xl_91011ffsxqybphcj40000gn/T/ipykernel_79070/2733187376.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     12\u001B[0m         \u001B[0mprediction\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimagesX\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m         \u001B[0mprediction\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mprediction\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdetach\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 14\u001B[0;31m         \u001B[0mmax_prob\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmax\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnumpy\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprediction\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     15\u001B[0m         \u001B[0mpredicted_label\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprediction\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mindex\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmax_prob\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mIndexError\u001B[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "# Test on ALL the images\n",
    "#tensor_image, label = next(iter(testloader)) # returns a batch of images\n",
    "\n",
    "correct_net = 0\n",
    "incorrect_net = 0\n",
    "\n",
    "for imagesX, labelsX in testloader:\n",
    "    print('')\n",
    "    for i in range(len(labelsX.tolist())):\n",
    "        first_image = numpy.array(imagesX, dtype='float')[i] # get the ith image in the batch\n",
    "\n",
    "        prediction = model(imagesX[i])\n",
    "        prediction = prediction.detach().numpy()\n",
    "        max_prob = max(list(numpy.array(prediction)[0]))\n",
    "        predicted_label = list(prediction[0]).index(max_prob)\n",
    "\n",
    "        print(f' {label[i].item()}/{predicted_label}', end = '')\n",
    "\n",
    "        if label[i].item() == predicted_label:\n",
    "            correct_net += 1\n",
    "            #print('+', end = '')\n",
    "        else:\n",
    "            incorrect_net += 1\n",
    "            #print(predicted_label, end = '') #let's take a look at what we're missing\n",
    "\n",
    "\n",
    "accuracy_net = round(correct_net * 100 / (correct_net + incorrect_net), 2)\n",
    "print('')\n",
    "print(f'Accuracy: {accuracy_net}% for {correct_net + incorrect_net} images.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "  <br> <br>**3. Train the model on only one epoch?** <br>"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}